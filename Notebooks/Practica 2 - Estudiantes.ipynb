{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1yJct12sdK2VlLPFF2WvTm2oK7AYYVpUG","timestamp":1632439944726},{"file_id":"1RNn3GVL3O9vKa_Euf2_IY95-m7Gi58mQ","timestamp":1631887093058},{"file_id":"1DTScby_a3yx36WxhyhZRKcVQAgx1iVdF","timestamp":1631210373349}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"sZSWq2lIEnbB"},"source":["# Práctica 2: Introducción a PyTorch\n","\n","## a) Calculando el gradiente mediante Autograd\n","\n","En primer lugar, vamos a calcular del gradiente para el perceptrón simple con función de activación sigmoidea que vimos en la teoría. Pero esta vez, en lugar de realizar manualmente el proceso de backpropagation, vamos a usar el módulo `autograd` de PyTorch.\n","\n","La función $f(x, w)$ a la cual queremos encontrarle el gradiente es:\n","\n","> $f(\\mathbf{x}, \\mathbf{w}) = \\frac{1}{1 + e^{2-(w_0 x_0 + w_1 x_1 + w_2)}}$\n","\n","Definimos entonces la función utilizando `torch.tensor` (recordar usar el parámetro `requires_grad = True` para que PyTorch guarde los gradientes) y realizamos la pasada \"forward\" para los siguientes valores de x y w:\n","\n","> $\\mathbf{x} = (-1, -2)$\n","\n","> $\\mathbf{w} = (2, -3, -3)$\n","\n"]},{"cell_type":"code","metadata":{"id":"UczyYh5Nj2u5"},"source":["import torch\n","\n","x = ...\n","w = ...\n","\n","f = ...\n","\n","print(f)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zkrbxHMukzHQ"},"source":["Ahora, utilizando la función `f.backward()` computamos los gradientes $\\frac{\\partial f}{ \\partial \\mathbf{x}}$ y $\\frac{\\partial f}{ \\partial \\mathbf{w}}$"]},{"cell_type":"code","metadata":{"id":"Q477bpjr77xp"},"source":["f....."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hXewlL_8YHMU"},"source":["print(\"Gradiente df/dx = \" + str( .... ))\n","print(\"Gradiente df/dw = \" + str( .... ))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XsXTQ9wJnK2j"},"source":["## b) Minimizando una función con Gradient Descent\n","\n","Ahora, vamos a implementar usar el algorítmo de gradiente descendiente (utilizando Autograd para computar el gradiente) para minimizar la función cuadrática $$f(x) = 2x^2 + x + 4$$\n","\n","Utilizaremos la implementación `torch.optim.SGD` de gradiente descendiente.\n","\n"]},{"cell_type":"code","metadata":{"id":"AKc75VsMYS4c"},"source":["import matplotlib.pyplot as plt\n","\n","# Definir la variable que será el parámetro a optimizar\n","x = torch.tensor(.....)\n","\n","# Definir el optimizador, indicando el parámetro a optimizar y el learning rate\n","optimizer = torch.optim.SGD( .... )\n","\n","# Acumuladores que usaremos para guardar los valores sucesivos de x, y\n","f_values = []\n","x_values = []\n","\n","# Loop de optimización\n","for i in range(1000):\n","\n","    # Setemos en 0 los gradientes de todos los elementos\n","    optimizer.....\n","\n","    # Pasada forward: ejecutar la función a minimizar\n","    f = .....\n","\n","    print(\"X = \" + str(x) + \", f(x) = \" + str(f))\n","\n","    # Pasada backward: computar los gradientes\n","    f......\n","\n","    # Actualizar los pesos dando un paso de gradiente descendiente\n","    optimizer....\n","\n","    # Guardar los valores para luego plotearlos\n","    f_values.append(f.data.item())\n","    x_values.append(x.data.item())\n","\n","# Ploteo los valores\n","plt.title(\"Optimizando la función f = 2 * x**2 + x + 4\")\n","plt.xlabel(\"X\")\n","plt.plot(x_values,f_values)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oTAAwWN2DWEH"},"source":["# c) Implementando un MLP en PyTorch para predicción del procentaje de grasa corporal\n","\n","Contamos con una base de datos [1] de 252 mediciones del porcentaje de grasa corporal de 252 personas diferentes, el cual puede ser estimado mediante otras características, como la edad, el peso, y mediciones en diferentes partes del cuerpo. A partir de estos datos se pretende desarrollar un sistema que permita predecir dicho porcentaje a partir de las características.\n","\n","[1]: Olson, R.S., La Cava, W., Orzechowski, P. et al. PMLB: a large benchmark suite for machine learning evaluation and comparison. BioData Mining 10, 36 (2017). https://epistasislab.github.io/pmlb/profile/560_bodyfat.html\n","\n","Antes de comenzar, vamos a instalar el paquete de Python que contiene la base de datos:"]},{"cell_type":"code","metadata":{"id":"GzDa1RLL3TUF"},"source":["pip install pmlb"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5ZoNrd9SEFQ9"},"source":["Ahora vamos a generar un histograma de todas las mediciones del porcentaje con todos los datos disponibles:"]},{"cell_type":"code","metadata":{"id":"-D-sOjKKSdmp"},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","from pmlb import fetch_data\n","\n","# Importamos el dataset\n","body_fat = fetch_data('560_bodyfat')\n","body_fat.describe()\n","\n","# Extraigo los datos (features) y los porcentajes (etiquetas a predecir)\n","data = body_fat.loc[:, body_fat.columns != 'target'].to_numpy()\n","percentages = body_fat.loc[:, body_fat.columns == 'target'].to_numpy()\n","\n","data = data.astype(np.float32)\n","percentages = percentages.astype(np.float32)\n","\n","print(\"Fila de ejemplo:\")\n","print(list(body_fat.columns))\n","print(data[0,:])\n","\n","# Dibujo un histograma del porcentaje de grasa corporal usando todos los datos\n","_ = plt.hist( ... , 50, density=True, facecolor='g', alpha=0.75)\n","_ = plt.title(\"Porcentaje de grasa corporal\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rgc2qJKLmhsB"},"source":["Particionamos los datos en entrenamiento y prueba usando la función `sklearn.model_selection.train_test_split`"]},{"cell_type":"code","metadata":{"id":"5WWTSCnnVyRK"},"source":[" from sklearn.model_selection import train_test_split\n","\n"," # Particiono los datos en entrenamiento y prueba usando el método de scikitlearn\n"," X_train, X_test, y_train, y_test = train_test_split( .... , .... , test_size=0.33, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1_D3sJMCoB_K"},"source":["Ahora implementaremos en PyTorch un Perceptrón multicapa que usaremos como regresor del porcentaje de grasa corporal (ejemplo basado en el curso de [RPI](https://rpi.analyticsdojo.com/)).\n","\n","El perceptrón deberá contar con 3 capas:\n","- Las dos primeras con 200 neuronas, y deberán usar la función de activación ReLU.\n","- La última con una única neurona cuya salida sea un valor escalar que corresponda al porcentaje de grasa corporal estimado de la persona, que no deberá utilizar ninguna función de activación.\n","\n","Algunas clases de PyTorch que resultarán útiles para implementar el modelo, son:\n","- `torch.nn.Linear`: Implementa una capa totalmente conectada. Es necesario especificarle el número de parámetros de entrada y de salida.\n","- `torch.nn.functional.relu`: Implementa la función de activación ReLU.\n","\n","Además, utilizaremos el optimizador `torch.optim.Adam` y la función de pérdida `torch.nn.MSELoss` (error cuadrático medio).\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"91rAzYsjkAUa"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","# Tamaño del batch de entrenamiento\n","batch_size = 32\n","\n","# Tasa de aprendizaje inicial para el gradiente descendente\n","learning_rate = 1e-3\n","\n","class Net(torch.nn.Module):\n","    def __init__(self, input_features, size_hidden, n_output):\n","        super(Net, self).__init__()\n","        self.hidden1 = ...\n","        self.hidden2 = ...\n","        self.out = ...\n","\n","    def forward(self, x):\n","        x = ...\n","        return x\n","\n","# Definimos el modelo del perceptrón\n","net = Net( ... , ... , ... )\n","\n","# Construimos el optimizador, y le indicamos que los parámetros a optimizar\n","# son los del modelo definido: net.parameters()\n","\n","optimizer = torch.optim.Adam( ... , lr=learning_rate)\n","\n","# Definimos también la función de pérdida a utilizar\n","criterion = ...\n","\n","# Creamos el objeto dataset que empaqueta los array de numpy para que puedan\n","# ser leidos por PyTorch\n","dataset = TensorDataset(torch.from_numpy(X_train).clone(), torch.from_numpy(y_train).clone())\n","\n","# Creamos un loader iterable indicandole que debe leer los datos a partir de\n","# del dataset creado en el paso anterior. Este objeto puede ser iterado\n","# y nos devuelve de a un batch (x, y).\n","loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n","\n","# Número de épocas\n","num_epochs = 5000\n","\n","# Lista en la que iremos guardando el valor de la función de pérdida en cada\n","# etapa de entrenamiento\n","loss_list = []\n","\n","# Bucle de entrenamiento\n","for i in range(num_epochs):\n","\n","    total_loss = 0.0\n","    for x, y in loader:\n","        # Seteo en cero los gradientes de los parámetros a optimizar\n","        optimizer...\n","\n","        # Realizo la pasada forward por la red\n","        loss = ...\n","\n","        # Realizo la pasada backward por la red\n","        loss....\n","\n","        # Actualizo los pesos de la red con el optimizador\n","        optimizer....\n","\n","        # Me guardo el valor actual de la función de pérdida para luego graficarlo\n","        loss_list.append(loss.data.item())\n","\n","        # Acumulo la loss del minibatch\n","        total_loss += loss.item() * y.size(0)\n","\n","    # Normalizo la loss total\n","    total_loss/= len(loader.dataset)\n","\n","    # Muestro el valor de la función de pérdida cada 100 iteraciones\n","    if i > 0 and i % 100 == 0:\n","        print('Epoch %d, loss = %g' % (i, total_loss))\n","\n","# Muestro la lista que contiene los valores de la función de pérdida\n","# y una versión suavizada (rojo) para observar la tendencia\n","plt.figure()\n","loss_np_array = np.array(loss_list)\n","plt.plot(loss_np_array, alpha = 0.3)\n","N = 60\n","running_avg_loss = np.convolve(loss_np_array, np.ones((N,))/N, mode='valid')\n","plt.plot(running_avg_loss, color='red')\n","plt.title(\"Función de pérdida durante el entrenamiento\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FhL0-b9J6eoD"},"source":["from sklearn.linear_model import LinearRegression\n","from numpy.linalg import norm\n","\n","# Definimos un método para mostrar las predicciones como un scatter plot\n","# y graficamos la recta de regresión para esos datos.\n","def plotScatter(x_data, y_data, title, fit_line=True):\n","  plt.figure()\n","\n","  plt.plot(x_data, y_data, '+')\n","  plt.xlabel('Valor real')\n","  plt.ylabel('Predicción')\n","  plt.title(title)\n","\n","  if fit_line:\n","    X, Y = x_data.reshape(-1,1), y_data.reshape(-1,1)\n","    plt.plot( X, LinearRegression().fit(X, Y).predict(X) )\n","\n","# Dibujamos el ground truth vs las predicciones en los datos de entrenamiento\n","py = net(torch.FloatTensor(X_train))\n","y_pred_train = py.cpu().detach().numpy()\n","plotScatter(y_train, y_pred_train, \"Training data\")\n","\n","# Dibujamos el ground truth vs las predicciones en los datos de test\n","py = net(torch.FloatTensor(X_test))\n","y_pred_test = py.cpu().detach().numpy()\n","plotScatter(y_test, y_pred_test, \"Test data\")\n","\n","print (\"MSE medio en training: \" + str(((y_train - y_pred_train)**2).mean()))\n","print (\"MSE medio en test: \" + str(((y_test - y_pred_test)**2).mean()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tc8fFGSm-D99"},"source":["# Entregable\n","1. Encontrar el mínimo de la función *f* definida en el apartado b). Para ello, deberán encontrar primero la derivada *f'(x)* de forma analítica, y utilizarla para computar el mínimo de la función. Posteriormente, deberán corrobarar que el valor coincida con el que obtuvieron optimizando la función con gradiente descendiente.\n","\n","2. Compara el rendimiento de 3 perceptrones multicapa que varíen en la cantidad de neuronas en sus capas intermedia. Probar colocando 2, 10 y 200 neuronas en dichas capas, al entrenar los perceptrones durante 5000 épocas. Mostrar los resultados utilizando:\n","\n","* los gráficos de dispersión con la recta de regresión\n","* el error medio en los datos de entrenamiento y test\n","\n","  Analizar la relación entre dichos resultados y la cantidad de neuronas que posee el perceptrón.\n",""]},{"cell_type":"code","metadata":{"id":"VMOqHl01-Frs"},"source":["\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7vZQ9moPMTSH"},"source":["\n"],"execution_count":null,"outputs":[]}]}